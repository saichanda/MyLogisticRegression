{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### \"\"\"Implementation of mYLogisticRegression using Gradient Descent Technique\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binary-Logistic Regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import numpy for functions like norm, Matmul...\n",
    "import numpy as np\n",
    "# Import loadmat from scipy for loading .mat file as a dictionary...\n",
    "from scipy.io import loadmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function for sigmoid or Logistic function...\n",
    "def mYLogisticFunction(X,theta):\n",
    "    \"\"\"This function returns the probability of x belonging to the true class\"\"\"\n",
    "    return 1/(1+(np.exp(-(np.matmul(theta,X.T)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Function for computing the Gradient Descent...\n",
    "def Derivative(X,theta,y):\n",
    "    \"\"\"This function returns the optimum cost function, which is used for computing the optimum theta\"\"\"\n",
    "    return np.matmul(X.T,(np.array(mYLogisticFunction(X,theta)) - y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Main program...\n",
    "# Load the Training data and their labels into X and y\n",
    "X = loadmat('mnistTrainImages.mat')\n",
    "y = loadmat('mnistTrainLabels.mat')\n",
    "# Save the 'trainData' into X_train from the dictionary 'X' and 'trainLabels' into y_train from dictionary 'y'\n",
    "X_train = X['trainData']\n",
    "y_train = y['trainLabels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The above Similar steps applied for test data and their labels\n",
    "XTest = loadmat('mnistTestImages.mat')\n",
    "yTest = loadmat('mnistTestLabels.mat')\n",
    "X_Test = XTest['testData']\n",
    "y_Test = yTest['testLabels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert y_train, which is list of lists into numpy array\n",
    "y_train = np.array([i[0] for i in y_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Introduce the bias term to account for the intercept\n",
    "X_train = np.insert(X_train,0,1,axis = 1)\n",
    "X_Test = np.insert(X_Test,0,1,axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize the parameters of the model\n",
    "theta = np.ones(X_train[0].shape)\n",
    "thetaOld = theta * 9999\n",
    "# The learning rate\n",
    "alpha = 0.05\n",
    "# Calculate the norm using np.linalg.norm()\n",
    "Norm = np.linalg.norm(theta - thetaOld,ord = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error Tolerance:  14.666470294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:4: RuntimeWarning: overflow encountered in exp\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "# Find the optimum theta iteratively\n",
    "while 1:\n",
    "    thetaOld = theta\n",
    "    theta = thetaOld - (alpha * Derivative(X_train,theta,y_train))\n",
    "    if Norm < np.linalg.norm(theta - thetaOld, ord = 2):\n",
    "        break\n",
    "    else:\n",
    "        Norm = np.linalg.norm(theta - thetaOld, ord = 2)\n",
    "print \"Error Tolerance: \", Norm\n",
    "# Here, we find the error tolerance ADAPTIVELY, instead of fixing it before hand,\n",
    "# inorder to avoid the local minimum to some extent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:4: RuntimeWarning: overflow encountered in exp\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "# Use theta computed above to calculate/ compute probabilities for the test dataset\n",
    "predictions = mYLogisticFunction(X_Test, theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use a suitable P to predict values and check the prediction on the test data\n",
    "for i in range(predictions.size):\n",
    "    if predictions[i] > 0.5:\n",
    "        predictions[i] = 1\n",
    "    else:\n",
    "        predictions[i] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is  0.9907\n"
     ]
    }
   ],
   "source": [
    "# Find the accuracy of our method\n",
    "count = 0\n",
    "for i in range(y_Test.size):\n",
    "    if predictions[i] == y_Test[i]:\n",
    "        count = count +1\n",
    "print \"Accuracy is \", count/float(y_Test.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Alpha ( Learning Rate) | Error_Tolerance | __init__Theta[i] | Accuracy |\n",
    "| :---: |:----------: | :----: | :---: |\n",
    "| 0.1 | 38.26  | 9999 | 99 |\n",
    "| 0.1 | 27979.62| 999 | 90.2 |\n",
    "| 0.1 | 2747.5 | 99 | 90.2 |\n",
    "| 0.05 | 14.67  | 9999 | 99 |\n",
    "| 0.05 | 2747.5| 999 | 99 |\n",
    "| 0.05 | 2747.5 | 99 | 90.2 |\n",
    "| 0.01 | 497.13  | 9999 | 90.2 |\n",
    "| 0.01 | 497.13| 999 | 90.2 |\n",
    "| 0.01 | 2747.5 | 99 | 90.2 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For different values of learning rate Alpha and the parameter initial theta, we have varied accuracies... along with considerable changes in the error tolerances. Due to the Oscillations in the model, we don't observe a good error tolerance in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Python built-in package for Logistic regression gives the accuracy of 99.23, which is little better than our method which gives 99 accuracy with learning rate of 0.05 and error tolerance of 19."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With respect to training and test accuracies:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Type of data | sklearn.linear_model |Our Method|\n",
    "|:-:|:--:|:--:|\n",
    "|Training|99.41|82.34|\n",
    "|Test|99.23|99.07|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that our method, though gives less accuracies on training set, performs well with the test data. Where as the linear_model of sklearn also does equally well by getting test accuracy close to that of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
