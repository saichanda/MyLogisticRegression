{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "###### \"\"\"Implementation of mYKclassLogisticRegression using Gradient Descent Technique\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KClass- Logistic Regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import numpy for functions like norm, Matmul...\n",
    "import numpy as np\n",
    "# Import loadmat from scipy for loading .mat file as a dictionary...\n",
    "from scipy.io import loadmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function for sigmoid or Logistic function...\n",
    "def mYLogisticFunction(X,theta):\n",
    "    \"\"\"This function returns the probability of x belonging to the true class\"\"\"\n",
    "    return 1.0/(1+(np.exp(-(np.matmul(theta,X.T)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Function for computing the Gradient Descent...\n",
    "def Derivative(X,theta,y):\n",
    "    \"\"\"This function returns the optimum cost function, which is used for computing the optimum theta\"\"\"\n",
    "    return np.matmul(X.T,(np.array(mYLogisticFunction(X,theta)) - y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Main program...\n",
    "# Load the Training data and their labels into X and y\n",
    "X = loadmat('mnistTrainImages.mat')\n",
    "y = loadmat('mnistTrainLabels.mat')\n",
    "# Save the 'trainData' into X_train from the dictionary 'X' and 'trainLabels' into y_train from dictionary 'y'\n",
    "X_train = X['trainData']\n",
    "y_train = y['trainLabels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The above Similar steps applied for test data and their labels\n",
    "XTest = loadmat('mnistTestImages.mat')\n",
    "yTest = loadmat('mnistTestLabels.mat')\n",
    "X_Test = XTest['testData']\n",
    "y_Test = yTest['testLabels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert y_Test,y_train, which are list of lists into numpy arrays\n",
    "y_train = np.array([i[0] for i in y_train])\n",
    "y_Test = np.array([i[0] for i in y_Test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Introduce the bias term to account for the intercept\n",
    "X_train = np.insert(X_train,0,1,axis = 1)\n",
    "X_Test = np.insert(X_Test,0,1,axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize the parameters of the model\n",
    "theta = []\n",
    "thetaOld = []\n",
    "Norm = []\n",
    "# Calculate the norm using np.linalg.norm()\n",
    "for i in range(10):\n",
    "    theta.append(np.ones(X_train[0].shape))\n",
    "    thetaOld.append(theta[i] * 99999)\n",
    "    Norm.append(np.linalg.norm(theta[i] - thetaOld[i],ord = 2))\n",
    "\n",
    "# The learning rate\n",
    "alpha = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Convert the training labels into list of lists with positive class as 1 and rest all classes as 0.\n",
    "Labels = []\n",
    "for i in range(10):                  \n",
    "     temp = np.copy(y_train)\n",
    "     temp[temp != i] = 10\n",
    "     temp[temp == i] = 1\n",
    "     temp[temp == 10] = 0\n",
    "     Labels.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:4: RuntimeWarning: overflow encountered in exp\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error Tolerance:  14.7039231513\n",
      "Error Tolerance:  14.6336165051\n",
      "Error Tolerance:  14.9905030406\n",
      "Error Tolerance:  14.81547044\n",
      "Error Tolerance:  14.6885062414\n",
      "Error Tolerance:  49.6701987499\n",
      "Error Tolerance:  14.7902685621\n",
      "Error Tolerance:  398.802849565\n",
      "Error Tolerance:  235.996574706\n",
      "Error Tolerance:  199.931817297\n"
     ]
    }
   ],
   "source": [
    "# Find the optimum theta for all classes iteratively\n",
    "for i in range(10):\n",
    "    while 1:\n",
    "        thetaOld[i] = theta[i]\n",
    "        theta[i] = thetaOld[i] - (alpha * Derivative(X_train,theta[i],Labels[i]))\n",
    "        if Norm[i] < np.linalg.norm(theta[i] - thetaOld[i], ord = 2) or Norm[i] < 15:\n",
    "            break\n",
    "        else:\n",
    "            Norm[i] = np.linalg.norm(theta[i] - thetaOld[i], ord = 2)\n",
    "    print \"Error Tolerance: \", Norm[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:4: RuntimeWarning: overflow encountered in exp\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "# Use theta values computed above for each class to calculate/ compute probabilities for the test dataset\n",
    "predictions = []\n",
    "for i in range(10):\n",
    "    predictions.append(mYLogisticFunction(X_train,theta[i]))\n",
    "# Find the predicted label by choosing the max probability of that class using np.argmax()\n",
    "predicted = np.argmax(predictions,axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is  0.72695\n"
     ]
    }
   ],
   "source": [
    "# Find the accuracy of the method\n",
    "count = 0\n",
    "for i in range(y_train.size):\n",
    "    if predicted[i] == y_train[i]:\n",
    "        count = count +1\n",
    "print \"Accuracy is \", count/float(y_train.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Alpha ( Learning Rate) | __init__Theta | Accuracy |\n",
    "| :---: | :----: | :---: |\n",
    "| 0.1 | 9999 | 70.4 |\n",
    "| 0.1 | 999 | 9.0 |\n",
    "| 0.1 | 99 | 9.0 |\n",
    "| 0.05 | 9999 | 71.09 |\n",
    "| 0.05 | 999 | 71.09 |\n",
    "| 0.05 | 99 | 9.0 |\n",
    "| 0.01 | 9999 | 73.8 |\n",
    "| 0.01 | 999 | 73.8 |\n",
    "| 0.01 | 99 | 9.0 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even in the case of K-Class Logistic Regression, For different values of learning rate Alpha and the parameter initial theta, we have varied accuracies... along with considerable changes in the error tolerances. However, due to the oscillations in the model, we don't observe a good error tolerance in the model. And the accuracy is approximately 74 for learning rate 0.01 with initial theta as high as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The python built-in package gives the accuracy of  , which is better than the method we proposed with learning rate 0.01."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with respect to training and test accuracies:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Type of data | sklearn.linear_model |Our Method|\n",
    "|:-:|:--:|:--:|\n",
    "|Training|92.80|72.70|\n",
    "|Test|92.01|73.8|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a similar situation of binary Logistic Regression, we observe that our method, though gives less accuracies on training set, but performs well with the test data. Where as the linear_model of sklearn also does equally well by getting test accuracy close to that of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
